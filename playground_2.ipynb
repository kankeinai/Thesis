{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook \\#2 - Implementation of Fourier Neural Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this beginning notebook, I am going to construct a framework for learning neural operators using the Fourier Neural Operator (FNO) architecture and solve a Diffusion Problem (to be explained later). The structure of this notebook goes as follows:\n",
    "- Implement a class for the FNO architecture\n",
    "- Implement a function for training the FNO with given differential equations and initial/boundary conditions\n",
    "- Generate a training dataset for the input control functions and the corresponding analytical solutions of the differential equation\n",
    "- Define loss functions associated with physics loss, initial, and boundary conditions\n",
    "- Train the FNO\n",
    "- Solve the Feldbaum Problem\n",
    "\n",
    "Let's start by implementing the Fourier Neural Operator architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.fft import fft, ifft\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from data import MultiFunctionDatasetODE, custom_collate_ODE_fn \n",
    "import plotter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components of the FNO Architecture\n",
    "\n",
    "### 1. Input Lifting with an MLP\n",
    "\n",
    "- **Purpose:**  \n",
    "  The first step is to \"lift\" the input data—typically low-dimensional features (e.g., time `t` and a function `u`)—into a higher-dimensional feature space. This is done using a Multi-Layer Perceptron (MLP).\n",
    "\n",
    "- **Implementation:**  \n",
    "  An MLP is built using a list of layer sizes (e.g., `[2, n_hid, n_hid]`) and corresponding activation functions (e.g., `['tanh', 'tanh']`). This network transforms the concatenated input `(t, u)` into a lifted representation suitable for spectral processing.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Spectral Convolution with Fourier Layers\n",
    "\n",
    "- **Fourier Transform:**  \n",
    "  The FNO converts the lifted input into the frequency domain using the Fast Fourier Transform (FFT). For example, in the code:\n",
    "\n",
    "```python\n",
    "  x_ft = fft(x, dim=-1)\n",
    "```\n",
    "#### Learnable Spectral Filtering:\n",
    "In the frequency domain, a subset of the Fourier modes (specifically the lower modes, which capture the most significant features) is filtered using learnable weights. This is achieved via an efficient tensor contraction with torch.einsum:\n",
    "out_ft[:, :, :self.modes] = torch.einsum(\"bcm,cio->bio\", x_ft[:, :, :self.modes], self.weights)\n",
    "Here:\n",
    "```python\n",
    "x_ft[:, :, :self.modes] represents the selected Fourier modes.\n",
    "```\n",
    "`self.weights` are learnable parameters with shape `(in_channels, out_channels, modes)`.\n",
    "#### Inverse Fourier Transform:\n",
    "After applying the spectral filter, the data is transformed back to the spatial domain using the inverse FFT:\n",
    "```python\n",
    "x_out = ifft(out_ft, dim=-1)\n",
    "return x_out.real\n",
    "```\n",
    "Only the real part of the inverse transform is used.\n",
    "#### Stacking Layers:\n",
    "Multiple Fourier layers can be stacked to enhance the model’s capacity. An optional nonlinearity (such as tanh) can be applied after each Fourier layer.\n",
    "\n",
    "### 3. Output Projection with an MLP\n",
    "***Purpose:***\n",
    "After spectral convolution, the data is still in a high-dimensional space. An output MLP projects this representation back to the desired output dimension (typically a scalar per input point).\n",
    "***Implementation:***\n",
    "Similar to the input MLP, the output MLP is built using a list of layer sizes (e.g., `[n_hid, n_hid, 1]`) and corresponding activation functions (e.g., `['tanh', None]`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 0.5036, initial_loss: 0.304875, physics_loss: 0.198755, time: 22:35:54.651937\n",
      "Epoch [2/2000], Loss: 0.2249, initial_loss: 0.103751, physics_loss: 0.121119, time: 22:36:02.737839\n",
      "Epoch [3/2000], Loss: 0.1865, initial_loss: 0.048871, physics_loss: 0.137599, time: 22:36:10.817737\n",
      "Epoch [4/2000], Loss: 0.1435, initial_loss: 0.014448, physics_loss: 0.129072, time: 22:36:18.899300\n",
      "Epoch [5/2000], Loss: 0.0958, initial_loss: 0.017902, physics_loss: 0.077916, time: 22:36:26.984371\n",
      "Epoch [6/2000], Loss: 0.0552, initial_loss: 0.021138, physics_loss: 0.034050, time: 22:36:35.069128\n",
      "Epoch [7/2000], Loss: 0.0311, initial_loss: 0.008832, physics_loss: 0.022246, time: 22:36:43.149071\n",
      "Epoch [8/2000], Loss: 0.0875, initial_loss: 0.036867, physics_loss: 0.050596, time: 22:36:51.231694\n",
      "Epoch [9/2000], Loss: 0.0108, initial_loss: 0.002961, physics_loss: 0.007797, time: 22:36:59.339697\n",
      "Epoch [10/2000], Loss: 0.0744, initial_loss: 0.032724, physics_loss: 0.041679, time: 22:37:07.424309\n",
      "Epoch [11/2000], Loss: 0.0089, initial_loss: 0.001536, physics_loss: 0.007389, time: 22:37:15.578041\n",
      "Epoch [12/2000], Loss: 0.0157, initial_loss: 0.009014, physics_loss: 0.006645, time: 22:37:23.683329\n",
      "Epoch [13/2000], Loss: 0.0421, initial_loss: 0.012119, physics_loss: 0.029935, time: 22:37:31.780731\n",
      "Epoch [14/2000], Loss: 0.1940, initial_loss: 0.162983, physics_loss: 0.031026, time: 22:37:39.899481\n",
      "Epoch [15/2000], Loss: 0.0139, initial_loss: 0.002151, physics_loss: 0.011767, time: 22:37:47.989986\n",
      "Epoch [16/2000], Loss: 0.0037, initial_loss: 0.001024, physics_loss: 0.002650, time: 22:37:56.101777\n",
      "Epoch [17/2000], Loss: 0.0738, initial_loss: 0.025972, physics_loss: 0.047780, time: 22:38:04.176166\n",
      "Epoch [18/2000], Loss: 0.0082, initial_loss: 0.004927, physics_loss: 0.003310, time: 22:38:12.254658\n",
      "Epoch [19/2000], Loss: 0.0105, initial_loss: 0.002971, physics_loss: 0.007496, time: 22:38:20.331563\n",
      "Epoch [20/2000], Loss: 0.0058, initial_loss: 0.002249, physics_loss: 0.003559, time: 22:38:28.403276\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 229\u001b[0m\n\u001b[1;32m    226\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Start training with optional plotting enabled.\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m      \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrf_lb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrf_lb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrf_ub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrf_ub\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 164\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset, epochs, lr, batch_size, plot, m, grf_lb, grf_ub)\u001b[0m\n\u001b[1;32m    161\u001b[0m time_domain \u001b[38;5;241m=\u001b[39m time_domain\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    163\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 164\u001b[0m loss, initial_loss, physics_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_domain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    166\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[21], line 125\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(model, t, u)\u001b[0m\n\u001b[1;32m    122\u001b[0m x_pred \u001b[38;5;241m=\u001b[39m model(t, u)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Compute d(x)/dt using autograd.\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m dx_dt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    126\u001b[0m residual \u001b[38;5;241m=\u001b[39m dx_dt \u001b[38;5;241m-\u001b[39m (x_pred \u001b[38;5;241m-\u001b[39m u)\n\u001b[1;32m    128\u001b[0m physics_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(residual\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Thesis/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/Documents/Thesis/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.fft import fft, ifft\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------\n",
    "# Fourier Layer Definition\n",
    "# ------------------------\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes):\n",
    "        \"\"\"\n",
    "        Applies a Fourier transform to the input, uses a learnable spectral filter, and transforms back.\n",
    "        Parameters:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            modes (int): Number of Fourier modes to keep.\n",
    "        \"\"\"\n",
    "        super(FourierLayer, self).__init__()\n",
    "        self.modes = modes\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        # Weight shape: (in_channels, out_channels, modes)\n",
    "        self.weights = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, modes, dtype=torch.cfloat))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, in_channels, N)\n",
    "        x_ft = fft(x, dim=-1)  # Fourier transform: shape (B, in_channels, N)\n",
    "        B, C, N = x_ft.shape\n",
    "        # Initialize output tensor in Fourier space\n",
    "        out_ft = torch.zeros(B, self.weights.shape[1], N, device=x.device, dtype=torch.cfloat)\n",
    "        # Apply learned weights on the first `modes` Fourier modes\n",
    "        out_ft[:, :, :self.modes] = torch.einsum(\"bcm,cio->bio\", x_ft[:, :, :self.modes], self.weights)\n",
    "        # Return to physical space via inverse FFT (real part only)\n",
    "        x_out = ifft(out_ft, dim=-1)\n",
    "        return x_out.real\n",
    "\n",
    "# ------------------------\n",
    "# Flexible FNO Model with Residual Connections\n",
    "# ------------------------\n",
    "class FNO(nn.Module):\n",
    "    def __init__(self, input_net, fourier_layers_config, output_net,\n",
    "                 input_activations, output_activations, fourier_activation=None):\n",
    "        \"\"\"\n",
    "        Flexible Fourier Neural Operator.\n",
    "        Parameters:\n",
    "            input_net (list): List of integers for the input MLP architecture.\n",
    "            fourier_layers_config (list): List of (modes, width) tuples for each Fourier layer.\n",
    "            output_net (list): List of integers for the output MLP architecture.\n",
    "            input_activations (list): List of activation names (or None) for the input MLP.\n",
    "            output_activations (list): List of activation names (or None) for the output MLP.\n",
    "            fourier_activation (str or None): Optional activation (e.g. 'tanh') applied after each Fourier layer.\n",
    "        \"\"\"\n",
    "        super(FNO, self).__init__()\n",
    "        self.input_mlp = self.build_mlp(input_net, input_activations)\n",
    "        \n",
    "        # Create Fourier layers; here we use 2 layers as recommended.\n",
    "        self.fourier_layers = nn.ModuleList([\n",
    "            FourierLayer(width, width, modes) for (modes, width) in fourier_layers_config\n",
    "        ])\n",
    "        self.fourier_activation = self.get_activation(fourier_activation) if fourier_activation is not None else None\n",
    "\n",
    "        self.output_mlp = self.build_mlp(output_net, output_activations)\n",
    "\n",
    "    def build_mlp(self, layers, activations):\n",
    "        \"\"\"Builds an MLP from a list of layer sizes and corresponding activation names.\"\"\"\n",
    "        modules = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            if activations[i] is not None:\n",
    "                modules.append(self.get_activation(activations[i]))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def get_activation(self, act_name):\n",
    "        act_name = act_name.lower()\n",
    "        if act_name == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif act_name == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif act_name == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {act_name}\")\n",
    "\n",
    "    def forward(self, t, u):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Parameters:\n",
    "            t (Tensor): (B, seq_len) e.g. time.\n",
    "            u (Tensor): (B, seq_len) e.g. function input.\n",
    "        Returns:\n",
    "            Tensor: (B, seq_len) prediction.\n",
    "        \"\"\"\n",
    "        # Concatenate inputs to shape (B, seq_len, 2)\n",
    "        x = torch.cat((t.unsqueeze(-1), u.unsqueeze(-1)), dim=-1)\n",
    "        x = self.input_mlp(x)  # Lifting via input MLP -> (B, seq_len, width)\n",
    "        x = x.permute(0, 2, 1)  # Rearranging for Fourier layers: (B, width, seq_len)\n",
    "        \n",
    "        # Iterate through Fourier layers with residual connections\n",
    "        for layer in self.fourier_layers:\n",
    "            residual = x  # Store current input for residual connection\n",
    "            x = layer(x)\n",
    "            if self.fourier_activation is not None:\n",
    "                x = self.fourier_activation(x)\n",
    "            x = x + residual  # Residual connection\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # Back to (B, seq_len, width)\n",
    "        x = self.output_mlp(x)  # Project to output -> (B, seq_len, 1)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# ------------------------\n",
    "# Loss and Training Functions\n",
    "# ------------------------\n",
    "def compute_loss(model, t, u):\n",
    "    \"\"\"\n",
    "    Computes the total loss, including the PDE (physics) loss and initial condition loss.\n",
    "    Returns:\n",
    "        total_loss, initial_loss, physics_loss\n",
    "    \"\"\"\n",
    "    # Ensure t is a leaf tensor with gradients enabled.\n",
    "    t = t.clone().detach().requires_grad_(True).cuda()\n",
    "    x_pred = model(t, u)\n",
    "    \n",
    "    # Compute d(x)/dt using autograd.\n",
    "    dx_dt = torch.autograd.grad(x_pred, t, grad_outputs=torch.ones_like(x_pred), create_graph=True)[0]\n",
    "    residual = dx_dt - (x_pred - u)\n",
    "    \n",
    "    physics_loss = torch.mean(residual**2)\n",
    "    initial_loss = torch.mean((x_pred[0] - 1)**2)\n",
    "    total_loss = physics_loss + initial_loss\n",
    "\n",
    "    return total_loss, initial_loss, physics_loss\n",
    "\n",
    "def train(model, dataset, epochs=1000, lr=0.001, batch_size=64, plot=False, m=None, grf_lb=None, grf_ub=None):\n",
    "    \"\"\"\n",
    "    Trains the model on the given dataset.\n",
    "    Additional logging and model saving is done every few epochs.\n",
    "    Parameters:\n",
    "        model: The FNO model.\n",
    "        dataset: Dataset instance.\n",
    "        epochs (int): Number of epochs.\n",
    "        lr (float): Learning rate.\n",
    "        batch_size (int): Batch size.\n",
    "        plot (bool): Whether to run plotter tests.\n",
    "        m, grf_lb, grf_ub: Parameters for the plotter routines.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Enable anomaly detection for debugging\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    num_epochs = epochs  # for logging consistency\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        # Iterate over batches\n",
    "        for u, _, _, _, time_domain, _ in dataloader:\n",
    "            # Move data to GPU\n",
    "            u = u.clone().detach().float().cuda()\n",
    "            time_domain = time_domain.clone().detach().float().cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, initial_loss, physics_loss = compute_loss(model, time_domain, u)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, '\n",
    "              f'initial_loss: {initial_loss.item():.6f}, physics_loss: {physics_loss.item():.6f}, '\n",
    "              f'time: {datetime.now().time()}')\n",
    "        \n",
    "        # Save model every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "            model_filename = f'epochs_[{epoch}]_model_time_[{timestamp}]_loss_[{loss.item():.4f}].pth'\n",
    "            torch.save(model.state_dict(), f\"trained_models/fno/{model_filename}\") \n",
    "\n",
    "# ------------------------\n",
    "# Architecture hyperparameters for the flexible FNO\n",
    "# ------------------------\n",
    "n_hid = 128\n",
    "modes = 16         # Increased Fourier modes as recommended\n",
    "num_fourier_layers = 2  # Using 2 Fourier layers with residual connections\n",
    "\n",
    "# Define the input MLP architecture: 2 input features -> n_hid -> n_hid\n",
    "input_net = [2, n_hid, n_hid]\n",
    "input_activations = ['tanh', 'tanh']\n",
    "\n",
    "# Fourier layers configuration: list of (modes, width) tuples.\n",
    "fourier_layers_config = [(modes, n_hid) for _ in range(num_fourier_layers)]\n",
    "fourier_activation = 'tanh'\n",
    "\n",
    "# Define the output MLP architecture: n_hid -> n_hid -> 1\n",
    "output_net = [n_hid, n_hid, 1]\n",
    "output_activations = ['tanh', None]\n",
    "\n",
    "# Dataset parameters (adjust as needed)\n",
    "m = 200   \n",
    "n_functions = 10000\n",
    "grf_lb = 0.02\n",
    "grf_ub = 0.5\n",
    "end_time = 1.0\n",
    "num_domain = 200\n",
    "num_initial = 20\n",
    "\n",
    "# Assuming MultiFunctionDatasetODE is defined elsewhere\n",
    "dataset = MultiFunctionDatasetODE(\n",
    "    m=m,\n",
    "    n_functions=n_functions,\n",
    "    function_types=['grf', 'linear', 'sine', 'polynomial', 'constant'],\n",
    "    end_time=end_time,\n",
    "    num_domain=num_domain,\n",
    "    num_initial=num_initial,\n",
    "    grf_lb=grf_lb,\n",
    "    grf_ub=grf_ub\n",
    ")\n",
    "\n",
    "# Initialize the model and move it to GPU.\n",
    "model = FNO(input_net, fourier_layers_config, output_net,\n",
    "            input_activations, output_activations, fourier_activation).cuda()\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 2000\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "# Start training with optional plotting enabled.\n",
    "train(model, dataset, epochs=epochs, lr=lr, batch_size=batch_size,\n",
    "      plot=True, m=m, grf_lb=grf_lb, grf_ub=grf_ub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/30000], Loss: 188.1175, time: 23:14:33.447287\n",
      "Epoch [200/30000], Loss: 185.9235, time: 23:14:39.778012\n",
      "Epoch [300/30000], Loss: 183.7566, time: 23:14:46.081916\n",
      "Epoch [400/30000], Loss: 181.6159, time: 23:14:52.419596\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.fft import fft, ifft\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------\n",
    "# Objective Function (Batched Version)\n",
    "# ------------------------\n",
    "def objective_function(n, args):\n",
    "    \"\"\"\n",
    "    Compute the objective:\n",
    "      J(u) = 0.5 * ∫₀¹ (x² + u²) dt + penalty terms,\n",
    "    enforcing:\n",
    "      dx/dt - (x - u) = 0  and  x(0) = 1.\n",
    "    \n",
    "    Args:\n",
    "        n (int): Batch size.\n",
    "        args (dict): Contains:\n",
    "            'x': Predicted state tensor of shape (n, m),\n",
    "            'u': Control tensor of shape (n, m),\n",
    "            't': Time tensor of shape (n, m).\n",
    "    \n",
    "    Returns:\n",
    "        J (Tensor): The total objective (scalar).\n",
    "    \"\"\"\n",
    "    # Assuming the state x is scalar so dim_x = 1.\n",
    "    dim_x = 1\n",
    "    m = args['t'].shape[1]\n",
    "    dx = torch.zeros(n, m, dim_x, device=args['t'].device)\n",
    "    \n",
    "    # Compute time derivative for each batch element.\n",
    "    for b in range(n):\n",
    "        grad_val = torch.autograd.grad(\n",
    "            args['x'][b],\n",
    "            args['t'][b],\n",
    "            torch.ones_like(args['x'][b]),\n",
    "            create_graph=True,\n",
    "            allow_unused=True  # allow unused gradients\n",
    "        )[0]\n",
    "        # Substitute with zeros if grad_val is None.\n",
    "        if grad_val is None:\n",
    "            grad_val = torch.zeros_like(args['t'][b])\n",
    "        # Unsqueeze to make shape (m, 1) so it can be assigned to dx[b] which is (m, 1)\n",
    "        dx[b] = grad_val.unsqueeze(-1)\n",
    "    \n",
    "    # Squeeze to shape (n, m)\n",
    "    dx_dt = dx[:, :, 0]\n",
    "\n",
    "    # Physics residual: dx/dt - (x - u) should be 0.\n",
    "    physics = dx_dt - args['x'] + args['u']\n",
    "    physics_loss = torch.mean(physics**2)\n",
    "    \n",
    "    # Initial condition penalty: x(0) should equal 1.\n",
    "    initial_loss = torch.mean((args['x'][:, 0] - torch.ones(n, device=args['t'].device))**2)\n",
    "\n",
    "    # Cost functional: 0.5 * ∫₀¹ (x² + u²) dt computed via the trapezoidal rule.\n",
    "    cost_integral = torch.trapz((args['x']**2 + args['u']**2).squeeze(), args['t'].squeeze(), dim=-1)\n",
    "    cost = 0.5 * torch.mean(cost_integral)\n",
    "\n",
    "    # Total objective: cost + weighted penalty terms.\n",
    "    J = cost + 100 * physics_loss + 10 * initial_loss\n",
    "    return J\n",
    "\n",
    "# ------------------------\n",
    "# Gradient Descent Routine (Batched Version)\n",
    "# ------------------------\n",
    "def gradient_descent(n, alpha, m, num_epochs, model, objective_function):\n",
    "    \"\"\"\n",
    "    Minimizes the objective with respect to the control u.\n",
    "    \n",
    "    Args:\n",
    "        n (int): Batch size.\n",
    "        alpha (float): Learning rate.\n",
    "        m (int): Number of time points (sensors).\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        model (nn.Module): Your trained FNO model that computes x = model(t, u).\n",
    "        objective_function (function): Computes the cost given u, t, and model output.\n",
    "        \n",
    "    Returns:\n",
    "        u_opt (Tensor): Optimized control averaged over the batch (shape: [m]).\n",
    "    \"\"\"\n",
    "    # Initialize control u with shape (n, m)\n",
    "    u = torch.randn(n, m, device=device)\n",
    "    \n",
    "    # Create time grid of shape (n, m) by repeating the same 1D time vector\n",
    "    t = torch.linspace(0, end_time, m, device=device).unsqueeze(0).repeat(n, 1)\n",
    "    \n",
    "    u.requires_grad_(True)\n",
    "    t.requires_grad_(True)  # if you need gradients for t\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([u], lr=alpha, weight_decay=0.01)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        x = model(t, u)  # FNO model returns x with shape (n, m)\n",
    "        args = {'x': x, 'u': u, 't': t}\n",
    "        loss = objective_function(n, args)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, time: {datetime.now().time()}')\n",
    "\n",
    "    # Return the optimized control averaged over the batch (shape: (m,))\n",
    "    return torch.mean(u, axis=0)\n",
    "\n",
    "# ------------------------\n",
    "# Hyperparameters and Model Loading\n",
    "# ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n = 64       # Batch size (number of random functions)\n",
    "m = 200       # Number of time points (sensors)\n",
    "num_epochs = 30000\n",
    "alpha = 0.0001\n",
    "\n",
    "# (Assume your FNO model class definition is available)\n",
    "# Instantiate and load your trained FNO model.\n",
    "trained_model = FNO(input_net, fourier_layers_config, output_net,\n",
    "                    input_activations, output_activations, fourier_activation)\n",
    "trained_model.to(device)\n",
    "trained_model.load_state_dict(torch.load('trained_models/fno/epochs_[19]_model_time_[20250317-223828]_loss_[0.0058].pth', map_location=device))\n",
    "trained_model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# Run Gradient Descent to Optimize u\n",
    "# ------------------------\n",
    "u_opt = gradient_descent(n, alpha, m, num_epochs, trained_model, objective_function)\n",
    "print(\"Optimized control u(t):\", u_opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

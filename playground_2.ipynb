{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook \\#2 - Implementation of Fourier Neural Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [CUDAComplexFloatType [64, 64, 12]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 109\u001b[0m\n\u001b[1;32m     97\u001b[0m dataset \u001b[38;5;241m=\u001b[39m MultiFunctionDatasetODE(\n\u001b[1;32m     98\u001b[0m     m\u001b[38;5;241m=\u001b[39mm,\n\u001b[1;32m     99\u001b[0m     n_functions\u001b[38;5;241m=\u001b[39mn_functions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     grf_ub\u001b[38;5;241m=\u001b[39mgrf_ub\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m u, time_domain \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda(), time_domain\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 70\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_domain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[46], line 48\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(model, t, u)\u001b[0m\n\u001b[1;32m     45\u001b[0m x_pred \u001b[38;5;241m=\u001b[39m model(t, u)  \u001b[38;5;66;03m# Predict x(t)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Compute d(x)/dt using autograd\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m dx_dt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Compute PDE residual\u001b[39;00m\n\u001b[1;32m     51\u001b[0m residual \u001b[38;5;241m=\u001b[39m dx_dt \u001b[38;5;241m-\u001b[39m (x_pred \u001b[38;5;241m-\u001b[39m u)\n",
      "File \u001b[0;32m~/Documents/Thesis/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/Documents/Thesis/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [CUDAComplexFloatType [64, 64, 12]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from torch.fft import fft, ifft\n",
    "from data import MultiFunctionDatasetODE, custom_collate_ODE_fn\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes):\n",
    "        super(FourierLayer, self).__init__()\n",
    "        self.modes = modes  # Number of Fourier modes to keep\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        self.weights = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, modes, dtype=torch.cfloat))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ft = fft(x, dim=-1)  # Compute FFT\n",
    "        x_ft = x_ft.clone()  # Fix: Clone to prevent in-place modification\n",
    "        x_ft[:, :, :self.modes] = x_ft[:, :, :self.modes] * self.weights  # Apply learned filter\n",
    "        x_out = ifft(x_ft, dim=-1)  # Compute inverse FFT\n",
    "        return x_out.real  # Return real part\n",
    "\n",
    "class FNO(nn.Module):\n",
    "    def __init__(self, modes, width):\n",
    "        super(FNO, self).__init__()\n",
    "        self.fc0 = nn.Linear(2, width)  # Expecting (t, u) as input\n",
    "        self.fourier = FourierLayer(width, width, modes)  # Fourier transform layer\n",
    "        self.fc1 = nn.Linear(width, width)  \n",
    "        self.fc2 = nn.Linear(width, 1)  \n",
    "\n",
    "    def forward(self, t, u):\n",
    "        x = torch.cat((t.unsqueeze(-1), u.unsqueeze(-1)), dim=-1)  # Shape: (batch, seq_len, 2)\n",
    "        x = self.fc0(x)  # Shape: (batch, seq_len, width)\n",
    "        x = x.permute(0, 2, 1)  # Shape: (batch, width, seq_len) for Fourier transform\n",
    "        x = self.fourier(x)  # Fourier Layer output (batch, width, seq_len)\n",
    "        x = x.permute(0, 2, 1)  # Convert back to (batch, seq_len, width)\n",
    "        x = self.fc1(x)  # Shape: (batch, seq_len, width)\n",
    "        x = self.fc2(x)  # Shape: (batch, seq_len, 1)\n",
    "        return x.squeeze(-1)  # Final shape: (batch, seq_len)\n",
    "\n",
    "def compute_loss(model, t, u):\n",
    "    t = t.clone().detach().requires_grad_(True).cuda()  # Fix: Ensure requires_grad is set correctly\n",
    "\n",
    "    x_pred = model(t, u)  # Predict x(t)\n",
    "\n",
    "    # Compute d(x)/dt using autograd\n",
    "    dx_dt = torch.autograd.grad(x_pred, t, grad_outputs=torch.ones_like(x_pred), create_graph=True)[0]\n",
    "    \n",
    "    # Compute PDE residual\n",
    "    residual = dx_dt - (x_pred - u)\n",
    "    loss_pde = torch.mean(residual**2)\n",
    "\n",
    "    # Initial condition loss\n",
    "    loss_ic = torch.mean((x_pred[0] - 1) ** 2)\n",
    "\n",
    "    return loss_pde + loss_ic\n",
    "\n",
    "def train(model, dataset, epochs=1000, lr=0.001, batch_size=64):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for u, _, _, _, time_domain, _ in dataloader:\n",
    "            # Fix: Clone tensors to prevent in-place modification issues\n",
    "            u, time_domain = u.clone().detach().float().cuda(), time_domain.clone().detach().float().cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_loss(model, time_domain, u)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Hyperparameters\n",
    "modes = 12\n",
    "width = 64\n",
    "epochs = 2000\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "m = 200   \n",
    "\n",
    "# Initialize model\n",
    "model = FNO(modes, width).cuda()\n",
    "\n",
    "# Dataset parameters\n",
    "n_functions = 10000\n",
    "grf_lb = 0.02\n",
    "grf_ub = 0.5\n",
    "end_time = 1.0\n",
    "num_domain = 200\n",
    "num_initial = 20\n",
    "\n",
    "dataset = MultiFunctionDatasetODE(\n",
    "    m=m,\n",
    "    n_functions=n_functions,\n",
    "    function_types=['grf', 'linear', 'sine', 'polynomial','constant'],\n",
    "    end_time=end_time,\n",
    "    num_domain=num_domain,\n",
    "    num_initial=num_initial,\n",
    "    grf_lb=grf_lb,\n",
    "    grf_ub=grf_ub\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "train(model, dataset, epochs=epochs, lr=lr, batch_size=batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

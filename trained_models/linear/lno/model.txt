modes = 8
width = 4
hidden_layer = 64

model = LNO1d(width, modes, hidden_layer=hidden_layer).to(device)

# ====================================
# Training settings
# ====================================
#Initialize Optimizer
lr = 0.001
print(f"Using learning rate: {lr}")
epochs = 1000

optimizer = optim.Adam(model.parameters(), lr=lr)

scheduler = StepLR(
    optimizer,
    step_size=50,   # decay LR every 50 epochs (set as you like)
    gamma=0.9,      # halve the LR
)


training(model, optimizer, scheduler, train_loader, test_loader, compute_loss, gradient_finite_difference, architecture=architecture, num_epochs=epochs, save = 10, save_plot = 10, early_stopping_patience=30, problem=problems[idx], w=[1, 1])
